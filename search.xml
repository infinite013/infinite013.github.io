<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CNN的反向传播-转载</title>
      <link href="/2020/04/07/cnn-de-fan-xiang-chuan-bo-zhuan-zai/"/>
      <url>/2020/04/07/cnn-de-fan-xiang-chuan-bo-zhuan-zai/</url>
      
        <content type="html"><![CDATA[<p>本文转载自：<a href="http://jermmy.xyz/2017/12/16/2017-12-16-cnn-back-propagation/" target="_blank" rel="noopener">http://jermmy.xyz/2017/12/16/2017-12-16-cnn-back-propagation/</a></p><p>在一般的全联接神经网络中，我们通过反向传播算法计算参数的导数。BP 算法本质上可以认为是链式法则在矩阵求导上的运用。但 CNN 中的卷积操作则不再是全联接的形式，因此 CNN 的 BP 算法需要在原始的算法上稍作修改。这篇文章主要讲一下 BP 算法在卷积层和 pooling 层上的应用。</p><h2 id="原始的-BP-算法"><a href="#原始的-BP-算法" class="headerlink" title="原始的 BP 算法"></a>原始的 BP 算法</h2><p>首先，用两个例子回顾一下原始的 BP 算法。</p><h3 id="最简单的例子"><a href="#最简单的例子" class="headerlink" title="最简单的例子"></a>最简单的例子</h3><p>先看一个最简单的例子：<img src="https://i.loli.net/2020/04/07/tud4NwTWaoM3FK7.png" alt="示例"><br>上图中，$a^l$表示第 $l$ 层的输出（$a^0$ 就是网络最开始的输入），网络的激活函数假设都是$σ()$，$w^l$ 和 $b^l$ 表示第$ l $层的参数，C 表示 loss function，$δ^l$ 表示第 $l$ 层的误差，$z^l$ 是第$ l $层神经元的输入，即 $z^l=w^la^{l−1}+b^l，a^l=σ(z^l)$。<br>接下来要用 BP 算法求参数的导数$\frac{\partial C}{\partial w}$和$\frac{\partial C}{\partial b}$。<br>$\delta^2=\frac{\partial C}{\partial z^2}=\frac{\partial C}{\partial a^2}\frac{\partial a^2}{\partial z^2}=\frac{\partial C}{\partial a^2}\sigma’(z^2)$<br>$\delta^1=\frac{\partial C}{\partial z^1}=\delta^2\frac{\partial z^2}{\partial a^1}\frac{\partial a^1}{\partial z^1}=\delta^2 w^2\sigma’(z^1)$<br>算出这两个误差项后，就可以直接求出导数了：<br>$\frac{\partial C}{\partial b^2}=\frac{\partial C}{\partial a^2}\frac{\partial a^2}{\partial z^2}\frac{\partial z^2}{\partial b^2}=\delta^2$<br>$\frac{\partial C}{\partial w^2}=\frac{\partial C}{\partial a^2}\frac{\partial a^2}{\partial z^2}\frac{\partial z^2}{\partial w^2}=\delta^2 a^1<br>$<br>$\frac{\partial C}{\partial b^1}$和$\frac{\partial C}{\partial w^1}$的求法是一样的，这里不再赘述。</p><h3 id="次简单的例子"><a href="#次简单的例子" class="headerlink" title="次简单的例子"></a>次简单的例子</h3><p>接下来稍微把网络变复杂一点：<img src="https://i.loli.net/2020/04/07/t9aPsc6NhgLdi3q.png" alt="示例"><br>符号的标记和上一个例子是一样的。要注意的是，这里的 $W^l$ 不再是一个数，而变成一个权重矩阵，$W_{kj}^l$ 表示第$ l−1 $层的第$ j$ 个神经元到第 $l$ 层的第 $k$ 个神经元的权值，如下图所示：<br><img src="https://i.loli.net/2020/04/07/9KkVR2inhdsxm6y.png" alt=""><br>首先，还是要先求出网络的误差 $δ$：$$\delta_1^2=\frac{\partial C}{\partial z_1^2}=\frac{\partial C}{\partial a_1^2}\sigma’(z_1^2) $$<br>$$\delta_2^2=\frac{\partial C}{\partial z_2^2}=\frac{\partial C}{\partial a_2^2}\sigma’(z_2^2)$$<br>由此得到：$$\delta^2=\begin{bmatrix} \delta_1^2 \ \delta_2^2 \end{bmatrix}=\begin{bmatrix} \frac{\partial C}{\partial a_1^2} \ \frac{\partial C}{\partial a_2^2} \end{bmatrix} \odot \begin{bmatrix} \sigma’(z_1^2) \ \sigma’(z_2^2) \end{bmatrix}$$<br>$\odot$代表对应元素相乘(elementwise)运算。<br>接着要根据 $δ^2$ 计算前一层的误差$ δ^1$：$\begin{align}<br>\delta_1^1=\frac{\partial C}{\partial z_1^1}=&amp;\frac{\partial C}{\partial a_1^2}\sigma’(z_1^2)\frac{\partial z_1^2}{\partial a_1^1}\frac{\partial a_1^1}{\partial z_1^1}+ \notag \<br>&amp;\frac{\partial C}{\partial a_2^2}\sigma’(z_2^2)\frac{\partial z_2^2}{\partial a_1^1}\frac{\partial a_1^1}{\partial z_1^1} \notag \<br>=&amp;\frac{\partial C}{\partial a_1^2}\sigma’(z_1^2)W_{11}^2\sigma’(z_1^1)+\tag{1} \<br>&amp;\frac{\partial C}{\partial a_2^2}\sigma’(z_2^2)W_{21}^2\sigma’(z_1^1) \notag \<br>=&amp;\begin{bmatrix}\frac{\partial C}{\partial a_1^2}\sigma’(z_1^2) &amp; \frac{\partial C}{\partial a_2^2}\sigma’(z_2^2)  \end{bmatrix} \begin{bmatrix} W_{11}^2  \ W_{21}^2 \end{bmatrix} \odot \begin{bmatrix} \sigma’(z_1^1) \end{bmatrix}  \notag<br>\end{align}$</p><p>同理，$\delta_2^1=\begin{bmatrix}\frac{\partial C}{\partial a_1^2}\sigma’(z_1^2) &amp; \frac{\partial C}{\partial a_2^2}\sigma’(z_2^2) \end{bmatrix} \begin{bmatrix} W_{12}^2 \\ W_{22}^2 \end{bmatrix} \odot \begin{bmatrix} \sigma’(z_2^1) \end{bmatrix}$<br>这样，我们就得到第 1 层的误差项：<br>$$\delta^1=<br>\begin{bmatrix}<br>W_{11}^2 &amp; W_{21}^2\\<br>W_{12}^2 &amp; W_{22}^2<br>\end{bmatrix}<br>\begin{bmatrix}<br>\frac{\partial C}{\partial z_1^2} \\<br>\frac{\partial C}{\partial z_2^2}<br>\end{bmatrix}<br>\odot<br>\begin{bmatrix}<br>\sigma’(z_1^1) \\<br>\sigma’(z_2^1)<br>\end{bmatrix}<br>={W^{2}}^T\delta^2 \odot \sigma’(z^1)  \tag{2}$$<br>然后，根据误差项计算导数：$$\frac{\partial C}{\partial b_j^2}=\frac{\partial C}{\partial z_j^2}\frac{\partial z_j^2}{\partial b_j^2}=\delta_j^2 \\<br>\frac{\partial C}{\partial w_{jk}^2}=\frac{\partial C}{\partial z_j^2}\frac{\partial z_j^2}{\partial w_{jk}^2}=a_k^{1}\delta_j^2 \\<br>\frac{\partial C}{\partial b_j^1}=\frac{\partial C}{\partial z_j^1}\frac{\partial z_j^1}{\partial b_j^1}=\delta_j^1 \\<br>\frac{\partial C}{\partial w_{jk}^1}=\frac{\partial C}{\partial z_j^1}\frac{\partial z_j^1}{\partial w_{jk}^1}=a_k^{0}\delta_j^1$$</p><h3 id="BP-算法的套路"><a href="#BP-算法的套路" class="headerlink" title="BP 算法的套路"></a>BP 算法的套路</h3><p>在 BP 算法中，我们计算的误差项$ δ^l $其实就是 loss function 对$ z^l $的导数 $\frac{\partial C}{\partial z^l}$，有了该导数后，根据链式法则就可以比较容易地求出 $\frac{\partial C}{\partial w^l}$ 和$\frac{\partial C}{\partial b^l}$。</p><h2 id="CNN中的BP算法"><a href="#CNN中的BP算法" class="headerlink" title="CNN中的BP算法"></a>CNN中的BP算法</h2><p>之所以要「啰嗦」地回顾普通的 BP 算法，主要是为了熟悉一下链式法则，因为这一点在理解 CNN 的 BP 算法时尤为重要。</p><p>下面就来考虑如何把之前的算法套路用在 CNN 网络中。</p><p>CNN 的难点在于卷积层和 pooling 层这两种很特殊的结构，因此下面重点分析这两种结构的 BP 算法如何执行。</p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>假设我们要处理如下卷积操作：<br>$$<br>\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\<br>a_{21}&amp;a_{22}&amp;a_{23}\\<br>a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right)    *  \left( \begin{array}{ccc} w_{11}&amp;w_{12}\\<br>w_{21}&amp;w_{22} \end{array} \right) = \left( \begin{array}{ccc} z_{11}&amp;z_{12}\\<br>z_{21}&amp;z_{22} \end{array} \right)$$<br>这个操作咋一看完全不同于全联接层的操作，这样，想套一下 BP 算法都不知从哪里入手。但是，如果把卷积操作表示成下面的等式，问题就清晰多了（卷积操作一般是要把卷积核旋转 180 度再相乘的，不过，由于 CNN 中的卷积参数本来就是学出来的，所以旋不旋转，关系其实不大，这里默认不旋转）：<br>$$z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} +   a_{22}w_{22} \\<br>z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} +   a_{23}w_{22} \\<br>z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} +   a_{32}w_{22} \\<br>z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} +   a_{33}w_{22}$$<br>更进一步，我们还可以把上面的等式表示成下图：<img src="https://i.loli.net/2020/04/07/x1DrkupbUSlnOa5.png" alt=""><br>上图的网络结构中，左边青色的神经元表示 $a_{11}$ 到$a_{33}$，中间橙色的表示$z_{11}$ 到 $z_{22}$。需要注意的是，青色和橙色神经元之间的权值连接用了不同的颜色标出，紫色线表示 $w_{11}$，蓝色线表示 $w_{12}$，依此类推。这样一来，如果你熟悉 BP 链式法则的套路的话，你可能已经懂了卷积层的 BP 是怎么操作的了。因为卷积层其实就是一种特殊的连接层，它是部分连接的，而且参数也是共享的。</p><h3 id="卷积层的误差项-δ-l−1"><a href="#卷积层的误差项-δ-l−1" class="headerlink" title="卷积层的误差项  $δ^l−1$"></a>卷积层的误差项  $δ^l−1$</h3><p>首先计算 $δ^l−1$。假设上图中的 $a^{l−1}$ 是前一层经过某些操作（可能是激活函数，也可能是 pooling 层等，但不管是哪种操作，我们都可以用 $σ()$ 来表示）后得到的响应，即 $a^{l−1}=σ(z^l−1)$。那么，根据链式法则：$$\delta^{l-1}=\frac{\partial C}{\partial z^{l-1}}=\frac{\partial C}{\partial z^{l}}\frac{\partial z^l}{\partial a^{l-1}}\frac{\partial a^{l-1}}{\partial z^{l-1}}=\delta^l \frac{\partial z^l}{\partial a^{l-1}} \odot \sigma’(z^{l-1}) \tag{3}$$<br>对照上面的例子，$z^l−1$ 应该是一个 9 维的向量，所以 $σ′(z^{l−1})$ 也是一个向量，根据之前 BP 的套路，这里需要 $\odot$ 操作。<br>这里的重点是要计算 $\frac{\partial z^l}{\partial a^{l-1}}$，这也是卷积层区别于全联接层的地方。根据前面展开的卷积操作的等式，这个导数其实比全联接层更容易求。以 $a_{11}^{l-1}$ 和 $a_{12}^{l-1}$为例（简洁起见，下面去掉右上角的层数符号 $l$）：$$\begin{align}<br>\nabla a_{11} = &amp; \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial a_{11}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{11}} \notag \\<br>=&amp; \delta_{11}w_{11} \notag \end{align}$$<br>$$\begin{align}<br>\nabla a_{12} =&amp; \frac{\partial C}{\partial z_{11}}\frac{\partial z_{11}}{\partial a_{12}} + \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial a_{12}} + \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial a_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial a_{12}} \notag \\<br>=&amp;\delta_{11}w_{12} + \delta_{12}w_{11} \notag<br>\end{align}$$<br>（$\nabla a_{ij}$ 表示$\frac{\partial C}{\partial a_{ij}}$。如果这两个例子看不懂，证明对之前 BP 例子中的（1）式理解不够，请先复习普通的 BP 算法。）<br>其他$\nabla a_{ij}$的计算，道理相同。<br>之后，如果你把所有式子都写出来，就会发现，我们可以用一个卷积运算来计算所有 $\nabla a_{ij}^{l-1}$：<br>$$\left( \begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\ 0&amp;\delta_{11}&amp; \delta_{12}&amp;0 \\ 0&amp;\delta_{21}&amp;\delta_{22}&amp;0 \\ 0&amp;0&amp;0&amp;0 \end{array} \right) * \left( \begin{array}{ccc} w_{22}&amp;w_{21}\\ w_{12}&amp;w_{11} \end{array} \right)  = \left( \begin{array}{ccc} \nabla a_{11}&amp;\nabla a_{12}&amp;\nabla a_{13} \\ \nabla a_{21}&amp;\nabla a_{22}&amp;\nabla a_{23}\\ \nabla a_{31}&amp;\nabla a_{32}&amp;\nabla a_{33} \end{array} \right)$$<br>这样一来，（3）式可以改写为：<br>$\delta^{l-1}=\frac{\partial C}{\partial z^{l-1}}=\delta^l * rot180(W^l) \odot \sigma’(z^{l-1}) \tag{4}$<br>（4）式就是 CNN 中误差项的计算方法。注意，跟原始的 BP 不同的是，这里需要将后一层的误差 δl 写成矩阵的形式，并用 0 填充到合适的维度。而且这里不再是跟矩阵 WlT 相乘，而是先将 Wl 旋转 180 度后，再跟其做卷积运算。</p><h3 id="卷积层的导数-frac-partial-C-partial-w-l-和-frac-partial-C-partial-b-l"><a href="#卷积层的导数-frac-partial-C-partial-w-l-和-frac-partial-C-partial-b-l" class="headerlink" title="卷积层的导数 $\frac{\partial C}{\partial w^l}$ 和  $\frac{\partial C}{\partial b^l}$"></a>卷积层的导数 $\frac{\partial C}{\partial w^l}$ 和  $\frac{\partial C}{\partial b^l}$</h3><p>这两项的计算也是类似的。假设已经知道当前层的误差项 $δ^l$，参考之前 $\nabla a_{ij}$ 的计算，可以得到：$$\begin{align}<br>\nabla w_{11}=&amp;\frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial w_{11}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial w_{11}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial w_{11}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial w_{11}}   \notag \\<br>=&amp;\delta_{11}a_{11}+\delta_{12}a_{12}+\delta_{21}a_{21}+\delta_{22}a_{22} \notag<br>\end{align}$$<br>$$\begin{align}<br>\nabla w_{12}=&amp;\frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial w_{12}}+  \frac{\partial C}{\partial z_{12}}\frac{\partial z_{12}}{\partial w_{12}}+ \frac{\partial C}{\partial z_{21}}\frac{\partial z_{21}}{\partial w_{12}} + \frac{\partial C}{\partial z_{22}}\frac{\partial z_{22}}{\partial w_{12}}   \notag \\<br>=&amp;\delta_{11}a_{12}+\delta_{12}a_{13}+\delta_{21}a_{22}+\delta_{22}a_{23} \notag<br>\end{align}$$<br>其他 $\nabla w_{ij}$ 的计算同理。<br>跟 $\nabla a_{ij}$一样，我们可以用矩阵卷积的形式表示：<br>$$\left( \begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13}\\ a_{21}&amp;a_{22}&amp;a_{23}\\ a_{31}&amp;a_{32}&amp;a_{33} \end{array} \right) * \left( \begin{array}{ccc}  \delta_{11}&amp; \delta_{12}\\ \delta_{21}&amp;\delta_{22}\end{array} \right)   = \left( \begin{array}{ccc} \nabla w_{11}&amp;\nabla w_{12}\\ \nabla w_{21}&amp;\nabla w_{22} \end{array} \right)$$<br>这样就得到了 $\frac{\partial C}{\partial w^l}$ 的公式：$$\frac{\partial C}{\partial w^l}=a^{l-1}* \delta^l \tag{5}$$<br>对于 $\frac{\partial C}{\partial b^l}$，我参考了文末的链接，但对其做法仍然不太理解，我觉得在卷积层中，$\frac{\partial C}{\partial b^l}$和一般的全联接层是一样的，仍然可以用下面的式子得到：$$\frac{\partial C}{\partial b^l}=\delta^l \tag{6}$$理解不一定对，所以这一点上大家还是参考一下其他资料。</p><h2 id="pooling-层"><a href="#pooling-层" class="headerlink" title="pooling 层"></a>pooling 层</h2><p>跟卷积层一样，我们先把 pooling 层也放回网络连接的形式中：<img src="https://i.loli.net/2020/04/07/JGHofmp4LjFMs9P.png" alt=""><br>红色神经元是前一层的响应结果，一般是卷积后再用激活函数处理。绿色的神经元表示 pooling 层。很明显，pooling 主要是起到降维的作用，而且，由于 pooling 时没有参数需要学习，因此，当得到 pooling 层的误差项 $δ^l $后，我们只需要计算上一层的误差项 $δ^{l−1}$ 即可。要注意的一点是，由于 pooling 一般会降维，因此传回去的误差矩阵要调整维度，即 $upsample$。这样，误差传播的公式原型大概是：<br>$\delta^{l-1}=upsample(\delta^l) \odot \sigma’(z^{l-1})$<br>下面以最常用的 average pooling 和 max pooling 为例，讲讲 $upsample(δ^l)$ 具体要怎么处理。<br>假设 pooling 层的区域大小为 2×2，pooling 这一层的误差项为：$$\delta^l= \left( \begin{array}{ccc} 2 &amp; 8 \\ 4 &amp; 6 \end{array} \right)$$<br>首先，我们先把维度还原到上一层的维度：$$\left( \begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 8 &amp; 0  \\ 0 &amp; 4 &amp; 6 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0  \end{array} \right)$$<br>在 average pooling 中，我们是把一个范围内的响应值取平均后，作为一个 pooling unit 的结果。可以认为是经过一个 average() 函数，即$average(x)=\frac{1}{m}\sum_{k=1}^m x_k$,在本例中，$m=4$。则对每个 $x_k$ 的导数均为：$$\frac{\partial average(x)}{\partial x_k}=\frac{1}{m}$$<br>因此，对 average pooling 来说，其误差项为：$$\begin{align}<br>\delta^{l-1}=&amp;\delta^l \frac{\partial average}{\partial x} \odot \sigma’(z^{l-1}) \notag \\<br>=&amp;upsample(\delta^l) \odot \sigma’(z^{l-1}) \tag{7} \\<br>=&amp;\left( \begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\ 0.5&amp;0.5&amp;2&amp;2 \\ 1&amp;1&amp;1.5&amp;1.5 \\ 1&amp;1&amp;1.5&amp;1.5 \end{array} \right)\odot \sigma’(z^{l-1}) \notag<br>\end{align}$$<br>在 max pooling 中，则是经过一个 max() 函数，对应的导数为：<br>$\frac{\partial \max(x)}{\partial x_k}=\begin{cases} 1 &amp; if\ x_k=max(x) \\ 0 &amp; otherwise \end{cases}$<br>假设前向传播时记录的最大值位置分别是左上、右下、右上、左下，则误差项为：<br>$$\delta^{l-1}=\left( \begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\ 0&amp;0&amp; 0&amp;8 \\ 0&amp;4&amp;0&amp;0 \\ 0&amp;0&amp;6&amp;0 \end{array} \right) \odot \sigma’(z^{l-1})  \tag{8}$$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="http://jermmy.xyz/2017/12/16/2017-12-16-cnn-back-propagation/" target="_blank" rel="noopener">http://jermmy.xyz/2017/12/16/2017-12-16-cnn-back-propagation/</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode155题-最小栈</title>
      <link href="/2020/04/06/leetcode155-ti-zui-xiao-zhan/"/>
      <url>/2020/04/06/leetcode155-ti-zui-xiao-zhan/</url>
      
        <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p><img src="https://i.loli.net/2020/04/06/M7IHSh3fzVxB5aO.png" alt=""></p><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><pre class=" language-javascript"><code class="language-javascript"><span class="token comment" spellcheck="true">//解法一：使用辅助栈</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Stack<span class="token punctuation">;</span><span class="token keyword">class</span> <span class="token class-name">MinStack</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> Stack<span class="token operator">&lt;</span>Integer<span class="token operator">></span> stack<span class="token punctuation">;</span>    <span class="token keyword">private</span> Stack<span class="token operator">&lt;</span>Integer<span class="token operator">></span> min_stack<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//private Stack&lt;Integer> stack;</span>    <span class="token comment" spellcheck="true">/** initialize your data structure here. */</span>    <span class="token keyword">public</span> <span class="token function">MinStack</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stack</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        min_stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stack</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">push</span><span class="token punctuation">(</span>int x<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>stack<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>            min_stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token keyword">else</span><span class="token punctuation">{</span>            stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>x<span class="token operator">&lt;=</span>min_stack<span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                min_stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>stack<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span>min_stack<span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            min_stack<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> int <span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> stack<span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> int <span class="token function">getMin</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> min_stack<span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">/*解法2：无辅助线解题思路：当push（x），x &lt;= min 时先将min进栈，再将x进栈；也就是说在栈中的每一个min下面都存有前一个min（就是比当前min小的那个)本质上来说这两种方法没有什么区别，时间空间复杂度都是一样的*/</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Stack<span class="token punctuation">;</span><span class="token keyword">class</span> <span class="token class-name">MinStack</span> <span class="token punctuation">{</span>    <span class="token keyword">private</span> Stack<span class="token operator">&lt;</span>Integer<span class="token operator">></span> stack<span class="token punctuation">;</span>    <span class="token keyword">private</span> int min<span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//private Stack&lt;Integer> stack;</span>    <span class="token comment" spellcheck="true">/** initialize your data structure here. */</span>    <span class="token keyword">public</span> <span class="token function">MinStack</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stack</span><span class="token operator">&lt;</span><span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        min <span class="token operator">=</span> Integer<span class="token punctuation">.</span>MAX_VALUE<span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">push</span><span class="token punctuation">(</span>int x<span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>x<span class="token operator">&lt;=</span>min<span class="token punctuation">)</span><span class="token punctuation">{</span>            stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>min<span class="token punctuation">)</span><span class="token punctuation">;</span>            min <span class="token operator">=</span> x<span class="token punctuation">;</span>        <span class="token punctuation">}</span>        stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//如果输入的值比当前最小值小，将当前最小值也入栈，</span>                      <span class="token comment" spellcheck="true">//那么栈中就存有两个当前的最小值以及新的最小值</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">if</span><span class="token punctuation">(</span>stack<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">==</span>min<span class="token punctuation">)</span><span class="token punctuation">{</span>            min <span class="token operator">=</span> stack<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> int <span class="token function">top</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> stack<span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span>    <span class="token keyword">public</span> int <span class="token function">getMin</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        <span class="token keyword">return</span> min<span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>来源：leetcode<br>连接：<a href="https://leetcode-cn.com/problems/min-stack/" target="_blank" rel="noopener"></a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 栈与队列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>leetcode20题-有效的括号</title>
      <link href="/2020/04/04/leetcode20-ti-you-xiao-de-gua-hao/"/>
      <url>/2020/04/04/leetcode20-ti-you-xiao-de-gua-hao/</url>
      
        <content type="html"><![CDATA[<h1 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h1><p>给定一个只包括 ‘(‘，’)’，’{‘，’}’，’[‘，’]’ 的字符串，判断字符串是否有效。</p><p>有效字符串需满足：</p><pre><code>左括号必须用相同类型的右括号闭合。左括号必须以正确的顺序闭合。</code></pre><p>注意空字符串可被认为是有效字符串<br><img src="https://i.loli.net/2020/04/04/BTMDY1QtOvjkdn9.png" alt="示例"></p><h1 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h1><pre class=" language-java"><code class="language-java">#解法<span class="token number">1</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>util<span class="token punctuation">.</span>Stack<span class="token punctuation">;</span><span class="token keyword">class</span> <span class="token class-name">Solution</span> <span class="token punctuation">{</span>    <span class="token keyword">public</span> <span class="token keyword">boolean</span> <span class="token function">isValid</span><span class="token punctuation">(</span>String s<span class="token punctuation">)</span> <span class="token punctuation">{</span>        Stack<span class="token operator">&lt;</span>Character<span class="token operator">></span> stack <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Stack</span><span class="token operator">&lt;</span>Character<span class="token operator">></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> s<span class="token punctuation">.</span><span class="token function">length</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> i <span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">{</span>            <span class="token keyword">char</span> temp <span class="token operator">=</span> s<span class="token punctuation">.</span><span class="token function">charAt</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">==</span> <span class="token string">'('</span> <span class="token operator">||</span> temp <span class="token operator">==</span> <span class="token string">'{'</span> <span class="token operator">||</span> temp <span class="token operator">==</span> <span class="token string">'['</span><span class="token punctuation">)</span>                stack<span class="token punctuation">.</span><span class="token function">push</span><span class="token punctuation">(</span>temp<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token keyword">else</span><span class="token punctuation">{</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>stack<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">{</span>                    <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>                <span class="token keyword">char</span> top_char <span class="token operator">=</span> stack<span class="token punctuation">.</span><span class="token function">pop</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">==</span> <span class="token string">')'</span> <span class="token operator">&amp;&amp;</span> top_char<span class="token operator">!=</span><span class="token string">'('</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">==</span> <span class="token string">'}'</span> <span class="token operator">&amp;&amp;</span> top_char<span class="token operator">!=</span><span class="token string">'{'</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>                <span class="token keyword">if</span><span class="token punctuation">(</span>temp <span class="token operator">==</span> <span class="token string">']'</span> <span class="token operator">&amp;&amp;</span> top_char<span class="token operator">!=</span><span class="token string">'['</span><span class="token punctuation">)</span>                    <span class="token keyword">return</span> <span class="token boolean">false</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        <span class="token keyword">return</span> stack<span class="token punctuation">.</span><span class="token function">isEmpty</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>来源：力扣（LeetCode）<br>链接：<a href="https://leetcode-cn.com/problems/valid-parentheses" target="_blank" rel="noopener">https://leetcode-cn.com/problems/valid-parentheses</a></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据结构 </tag>
            
            <tag> 栈与队列 </tag>
            
            <tag> leetcode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CBAM-注意力机制</title>
      <link href="/2020/03/31/cbam-zhu-yi-li-ji-zhi/"/>
      <url>/2020/03/31/cbam-zhu-yi-li-ji-zhi/</url>
      
        <content type="html"><![CDATA[<h1 id="CBAM总览"><a href="#CBAM总览" class="headerlink" title="CBAM总览"></a>CBAM总览</h1><p>文章中提到的主要的贡献有以下三点：<br>1、提出了一个简单但是有效的注意力模块(CBAM)，这个模块可以被广泛的使用来增强CNN的表达能力。<br>2、通过消融实验验证了注意力模块的有效性。<br>3、验证了在benchmarks上增加这个轻量模块可以增强实验结果。<br><img src="https://i.loli.net/2020/03/31/ICt7LHqbX5lYj8p.png" alt="CBAM模块结构图"><br>给定一个(C,H,W)的feature map作为输入，CBAM按顺序地生成1D的channel attention map C*1*1和2D的spatial attention map Ms 1*H*W，可以用下面这个公式来表达:<br><img src="https://i.loli.net/2020/03/31/UGIthuB4EJTHpXa.png" alt=""><br>下图表示了模块的具体结构，分为channel和spatial两部分：<br><img src="https://i.loli.net/2020/03/31/Pg8nDMCyQGioepd.png" alt="模块具体结构"></p><h1 id="Channel-attention-module"><a href="#Channel-attention-module" class="headerlink" title="Channel attention module"></a>Channel attention module</h1><p>作者通过features map通道间的关系来生成 channel attention map。Feature map 的每一个通道都被看作是一个feature detector，channel attention关注给定的图像中什么是有意义的。作者为了有效的计算channel attention，压缩了输入特征的图的spatial dimention（空间维度）。为了聚合（aggregate）空间信息，作者同时使用了平均池化以及最大池化。<br>    首先使用池化操作去聚合空间信息，生成两个不同的空间上下文描述符(spatial context descriptors)，然后将描述符送进一个共享的前向网络产生channel attention map Mc，他的维度为C*1*1。这个共享的网络是含有一个隐藏层的多层感知器multi-layer perceptron (MLP) 。通过隐藏层的设定减少了一些参数。之后对特征参数向量进行融合（将两个向量对应元素相加，如上图所示）。总之可以将channel attention表述为下式：<br><img src="https://i.loli.net/2020/03/31/Q13iq2BpVTvJM7A.png" alt=""></p><h1 id="Spatial-attention-module"><a href="#Spatial-attention-module" class="headerlink" title="Spatial attention module"></a>Spatial attention module</h1><p> 通过使用特征间的inter-spatial 关系来生成spatial attention map。作者说spatial attention关注于哪里是有信息的地方，与channel attention 互补。具体可以表示为下式：<img src="https://i.loli.net/2020/03/31/Z723kS8GrxpfvJB.png" alt=""><br> 作者通过实验验证了先通过channel attention module再通过spatial attention module效果最好。</p><h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><p> <a href="https://github.com/luuuyi/CBAM.PyTorch" target="_blank" rel="noopener">https://github.com/luuuyi/CBAM.PyTorch</a></p>]]></content>
      
      
      <categories>
          
          <category> cv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> 注意力机制 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fasterrcnn详解</title>
      <link href="/2020/03/31/fasterrcnn-xiang-jie/"/>
      <url>/2020/03/31/fasterrcnn-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="fasterrcnn结构概览"><a href="#fasterrcnn结构概览" class="headerlink" title="fasterrcnn结构概览"></a>fasterrcnn结构概览</h2><p><img src="https://i.loli.net/2020/03/31/3joDRnFCWf2zdks.png" alt="Faster RCNN结构图"><br>    上图为论文中的总体结构，在作者看来，主要分为四个部分：</p><ol><li><p>Conv layers。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。</p></li><li><p>Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于positive或者negative，再利用bounding box regression修正anchors获得精确的proposals。</p></li><li><p>Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。</p></li><li><p>Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。</p><p> 另一种结构图如下所示：<img src="https://i.loli.net/2020/03/31/eWSPnOvXcVEN3tu.png" alt="框架"><br> 表示为python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成positive anchors和对应bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。</p></li></ol><h2 id="RPN-Region-Proposal-Networks-到底是什么"><a href="#RPN-Region-Proposal-Networks-到底是什么" class="headerlink" title="RPN(Region Proposal Networks)到底是什么"></a>RPN(Region Proposal Networks)到底是什么</h2><p>   到底什么是RPN呢，他又起了什么作用呢？我们首先来看RPN的网络结构<img src="https://i.loli.net/2020/03/31/DuUgAoyveBfVlSq.png" alt="RPN结构"><br>    具体来说，我们可以将它分解为两条路径，上方的一条路径通过softmax对anchor进行一个<strong>二分类</strong>，区分positive和negative的anchor，下方路径计算anchor对于bbx的偏移量，获得精确的proposal，<strong>最后的Proposal层则负责综合positive anchors和对应bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能</strong>。<br>    其实RPN最终就是在原图尺度上，设置了密密麻麻的候选Anchor。然后用cnn去判断哪些Anchor是里面有目标的positive anchor，哪些是没目标的negative anchor。</p><h3 id="anchor"><a href="#anchor" class="headerlink" title="anchor"></a>anchor</h3><p>   提到RPN，就离不开anchor。所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形，直接运行mmdetection中的anchor生成代码，可以得到以下输出：<br>`[[ -84.  -40. 99.  55.]</p><p> [-176.  -88.  191.  103.]<br> [-360. -184.  375.  199.]<br> [ -56.  -56.   71.   71.]<br> [-120. -120.  135.  135.]<br> [-248. -248.  263.  263.]<br> [ -36.  -80.   51.   95.]<br> [ -80. -168.   95.  183.]<br> [-168. -344.  183.  359.]]</p><p> `<br><img src="https://i.loli.net/2020/03/31/MEV2By4XP91jZxw.png" alt="anchor示意图"><br>     这就是生成的anchor，其中每行的4个值(x1,y1,x2,y2)表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为 三种，如图6。实际上通过anchors就引入了检测中常用到的多尺度方法。<br>    对于维度是(W,H)的特征图来说，其生成的anchor个数为WxHx9个，这些anchor对应这特征图上的各个部分，然后对这些anchor进行判定，判定其中是否包含正样本(即包含可能的目标)<br>    当然，在获得positive anchor后，我们要对anchor进行微调，让他更接近真实的bbx，这就称作bounding box regression。</p><h3 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h3><p>如下图所示绿色框为飞机的Ground Truth(GT)，红色为提取的positive anchors，即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得positive anchors和GT更加接近。<br><img src="https://i.loli.net/2020/03/31/Z2yABCnGRmb6ruE.png" alt="GT与anchor对比图"><br>   对于窗口一般使用四维向量(x,y,w,h)表示，分别表示窗口的中心点坐标和宽高，对于下图，红色的框A代表原始的positive Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即：<br>  <img src="https://i.loli.net/2020/03/31/bMHV5RfU2Y6Nim3.png" alt=""><br>   那么经过何种变换F才能从图中的anchor A变为G’呢？ 比较简单的思路就是:<br><img src="https://i.loli.net/2020/03/31/si21RvB6QONFq4k.png" alt="变换"><br><img src="https://i.loli.net/2020/03/31/L78RpPwXVDZnlgd.png" alt=""><br><img src="https://i.loli.net/2020/03/31/6N9OJCjHFVy8mIx.png" alt=""><br><img src="https://i.loli.net/2020/03/31/iI1Pdljfm6TEkQM.png" alt=""></p><h2 id="模型训练中的损失函数"><a href="#模型训练中的损失函数" class="headerlink" title="模型训练中的损失函数"></a>模型训练中的损失函数</h2><p>在模型训练的过程中，如下图所示，<br>有两个Classification loss和两个Bounding-box regression loss，区别为：<br>Input Image经过CNN特征提取，首先来到Region Proposal网络。由Regio Proposal Network输出的Classification，这并不是判定物体在数据集上对应的类中哪一类，而是输出一个Binary的值p，可以理解为p在0，1之间 ，人工设定一个threshold=0.5。RPN网络做的事情就是，如果一个Region的 p&gt;=0.5，则认为这个Region中可能是80个类别中的某一类，具体是哪一类现在还不清楚。到此为止，Network只需要把这些可能含有物体的区域选取出来就可以了，这些被选取出来的Region又叫做ROI （Region of Interests），即感兴趣的区域。当然了，RPN同时也会在feature map上框定这些ROI感兴趣区域的大致位置，即Bounding-box。<br><img src="https://i.loli.net/2020/03/31/hAXHFNVTDkYv48S.png" alt="损失函数框图"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><pre class=" language-markdown"><code class="language-markdown"><span class="token blockquote punctuation">></span> [](https://zhuanlan.zhihu.com/p/31426458)<span class="token blockquote punctuation">></span> [](https://zhuanlan.zhihu.com/p/69250914)</code></pre><p>这是行内公式 $ \Gamma(z) = \int_0^\infty t^{z-1}e^{-t}dt, $</p>]]></content>
      
      
      <categories>
          
          <category> cv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cv </tag>
            
            <tag> fasterrcnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客上传</title>
      <link href="/2020/03/26/bo-ke-shang-chuan/"/>
      <url>/2020/03/26/bo-ke-shang-chuan/</url>
      
        <content type="html"><![CDATA[<p>在git bash下</p><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"新的博客"</span></code></pre><p>之后对新的博客.md进行编辑，修改对应的tag以及categories<br>修改完成之后可以对博客进行预览</p><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>上传的具体方法为</p><pre class=" language-bash"><code class="language-bash">$ hexo clean$ hexo g -d</code></pre><p>就大功告成了</p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
